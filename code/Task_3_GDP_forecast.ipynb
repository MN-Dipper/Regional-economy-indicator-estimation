{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc007c38-c386-4859-9c38-cc63dfd36aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b7ae90-d3dd-40c0-9a59-8eccfb33a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_classification = {\n",
    "    \"一线城市\": [\"北京\", \"上海\", \"广州\", \"深圳\"],\n",
    "    \"新一线城市\": [\"成都\", \"重庆\", \"杭州\", \"武汉\", \"西安\", \"天津\", \"苏州\", \"南京\", \"郑州\", \"长沙\", \"东莞\", \"沈阳\", \"青岛\", \"合肥\", \"佛山\"],\n",
    "    \"二线城市\": [\"宁波\", \"昆明\", \"福州\", \"无锡\", \"厦门\", \"济南\", \"大连\", \"温州\", \"泉州\", \"南宁\", \"南昌\", \"石家庄\", \"金华\", \"常州\", \"惠州\", \"嘉兴\", \"南通\", \"徐州\", \"绍兴\", \"台州\", \"太原\", \"珠海\", \"中山\", \"保定\", \"兰州\", \"哈尔滨\", \"长春\", \"贵阳\", \"烟台\", \"廊坊\"],\n",
    "    \"三线城市\": [\"潍坊\", \"海口\", \"扬州\", \"汕头\", \"洛阳\", \"乌鲁木齐\", \"临沂\", \"唐山\", \"镇江\", \"盐城\", \"湖州\", \"赣州\", \"济宁\", \"泰州\", \"呼和浩特\", \"咸阳\", \"芜湖\", \"三亚\", \"漳州\", \"揭阳\", \"江门\", \"桂林\", \"邯郸\", \"银川\", \"阜阳\", \"淮安\", \"衡阳\", \"遵义\", \"柳州\", \"淄博\", \"湛江\", \"上饶\", \"莆田\", \"绵阳\", \"商丘\", \"九江\", \"新乡\", \"信阳\", \"宜昌\", \"沧州\", \"连云港\", \"南阳\", \"岳阳\", \"襄阳\", \"蚌埠\", \"宁德\", \"邢台\", \"株洲\", \"周口\", \"马鞍山\", \"清远\", \"驻马店\", \"潮州\", \"滁州\", \"威海\", \"宿迁\", \"秦皇岛\", \"肇庆\", \"荆州\", \"宿州\", \"菏泽\", \"鞍山\", \"安庆\", \"黄冈\", \"泰安\", \"南充\", \"六安\", \"宜春\", \"大庆\", \"舟山\"],\n",
    "    \"四线城市\": [\"孝感\", \"丽水\", \"运城\", \"德州\", \"许昌\", \"常德\", \"渭南\", \"晋中\", \"安阳\", \"三明\", \"开封\", \"郴州\", \"茂名\", \"湘潭\", \"营口\", \"邵阳\", \"德阳\", \"龙岩\", \"淮南\", \"黄石\", \"南平\", \"韶关\", \"亳州\", \"日照\", \"西宁\", \"衢州\", \"东营\", \"吉林\", \"梅州\", \"聊城\", \"枣庄\", \"包头\", \"怀化\", \"宣城\", \"临汾\", \"盘锦\", \"锦州\", \"榆林\", \"抚州\", \"景德镇\", \"北海\", \"宝鸡\", \"焦作\", \"平顶山\", \"玉林\", \"十堰\", \"汕尾\", \"咸宁\", \"宜宾\", \"滨州\", \"永州\", \"吉安\", \"益阳\", \"黔南\", \"丹东\", \"曲靖\", \"乐山\", \"鄂尔多斯\", \"阳江\", \"泸州\", \"黄山\", \"黔东南\", \"张家口\", \"大理\", \"大同\", \"恩施\", \"衡水\", \"铜陵\", \"承德\", \"红河\", \"河源\", \"延边\", \"齐齐哈尔\", \"延安\", \"漯河\", \"葫芦岛\", \"娄底\", \"抚顺\", \"拉萨\", \"铜仁\", \"长治\", \"达州\", \"忻州\", \"鄂州\", \"淮北\", \"濮阳\", \"眉山\", \"池州\", \"荆门\", \"吕梁\"],\n",
    "    \"五线城市\": [\"汉中\", \"辽阳\", \"梧州\", \"鹰潭\", \"百色\", \"毕节\", \"钦州\", \"云浮\", \"佳木斯\", \"朝阳\", \"贵港\", \"四平\", \"丽江\", \"内江\",\"安顺\", \"三门峡\", \"赤峰\", \"六盘水\", \"新余\", \"牡丹江\", \"晋城\", \"自贡\", \"本溪\", \"防城港\", \"铁岭\", \"随州\", \"广安\", \"广元\", \"天水\", \"遂宁\", \"萍乡\", \"西双版纳\", \"绥化\", \"鹤壁\", \"湘西\",\"张家界\", \"松原\", \"阜新\", \"酒泉\", \"黔西南\", \"保山\", \"昭通\",\"贺州\", \"通化\", \"克拉玛依\", \"呼伦贝尔\", \"阳泉\", \"河池\", \"来宾\", \"通辽\", \"玉溪\", \"安康\", \"德宏\", \"楚雄\", \"朔州\", \"伊犁\", \"文山\", \"嘉峪关\", \"凉山\", \"资阳\", \"雅安\", \"锡林郭勒\", \"普洱\",\"崇左\", \"庆阳\",  \"巴音郭楞\", \"乌兰察布\", \"白山\",\"昌吉\", \"白城\",\"兴安\", \"定西\", \"喀什\", \"白银\", \"陇南\", \"张掖\", \"商洛\", \"哈密\", \"黑河\", \"吴忠\", \"攀枝花\", \"巴彦淖尔\", \"巴中\", \"鸡西\",\"乌海\", \"临沧\", \"海东\", \"双鸭山\", \"阿克苏\", \"石嘴山\", \"阿拉善\",\"辽源\", \"临夏\", \"铜川\", \"海西\", \"平凉\", \"金昌\", \"鹤岗\",\"林芝\", \"伊春\", \"固原\", \"武威\", \"儋州\", \"甘孜\", \"吐鲁番\",\"怒江\", \"和田\", \"迪庆\", \"甘南\", \"阿坝\", \"大兴安岭\", \"中卫\",\"塔城\", \"博尔塔拉\", \"昌都\", \"阿勒泰\", \"七台河\", \"山南\", \"日喀则\", \"玉树\", \"海南\", \"果洛\", \"克孜勒苏\", \"阿里\", \"海北\", \"黄南\", \"那曲\",\"三沙\"]\n",
    "\n",
    "}\n",
    "\n",
    "city_list = []\n",
    "for level, cities in city_classification.items():\n",
    "    for city in cities:\n",
    "        city_list.append({\"rev_addr_city_id\": city, \"city_level\": level})\n",
    "\n",
    "city_df = pd.DataFrame(city_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d591add6-14b5-4dfa-8b9d-2a02e6a8fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.read_csv('filter_data_2.csv')\n",
    "\n",
    "data_all = data_all.groupby(['rev_addr_city_id','item_first_cate_name', 'year'])['nums'].sum().reset_index()\n",
    "data_all = data_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c37e0903-a581-4bd6-b478-a565d2d0efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nan_gdp = data_all[data_all['nums'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204ddfb4-8a43-43bd-a08e-028ed85e1c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_all)\n",
    "\n",
    "pivot_df = df.pivot(index=['rev_addr_city_id', 'year'], columns='item_first_cate_name', values='nums').reset_index()\n",
    "pivot_df = pivot_df.fillna(0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7beeab68-d25f-4e72-81f2-f332214312b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(pivot_df)\n",
    "\n",
    "unique_cities = pd.unique(df1['rev_addr_city_id'])\n",
    "char_dict = defaultdict(list)\n",
    "for city in unique_cities:\n",
    "    char_dict[city[:2]].append(city)\n",
    "\n",
    "duplicates = {key: value for key, value in char_dict.items() if len(value) > 1}\n",
    "\n",
    "\n",
    "if duplicates:\n",
    "    cities_with_duplicates = [city for cities in duplicates.values() for city in cities]\n",
    "    has_duplicates = df1[df1['rev_addr_city_id'].isin(cities_with_duplicates)]\n",
    "    no_duplicates = df1[~df1['rev_addr_city_id'].isin(cities_with_duplicates)]\n",
    "else:\n",
    "    has_duplicates = pd.DataFrame(columns=df1.columns)\n",
    "    no_duplicates = df1.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ce5c331-cc5b-44de-8bd9-e6e3ecd7d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = no_duplicates.copy()\n",
    "df2 = city_df.copy()\n",
    "\n",
    "df1['city_prefix'] = df1['rev_addr_city_id'].str[:2]\n",
    "df2['city_prefix'] = df2['rev_addr_city_id'].str[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eb36f16-22e6-461d-8500-a9458226d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df = pd.merge(df1, df2, on=['city_prefix'], how='left')\n",
    "\n",
    "merged_df = merged_df.drop(columns=['city_prefix','rev_addr_city_id_y'])\n",
    "merged_df.rename(columns={'rev_addr_city_id_x': 'rev_addr_city_id'}, inplace=True)\n",
    "merged_df['city_level'] = merged_df['city_level'].fillna('六线城市')\n",
    "merged_1 = merged_df.copy()\n",
    "no_duplicates = merged_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f95fed3-df41-450e-b3b6-6f1423396d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = has_duplicates.copy()\n",
    "df2 = city_df.copy()\n",
    "\n",
    "df1['city_prefix'] = df1['rev_addr_city_id'].str[:3]\n",
    "df2['city_prefix'] = df2['rev_addr_city_id'].str[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6cab1c2-cb02-4ea9-a2cd-6b04fba6f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df = pd.merge(df1, df2, on=['city_prefix'], how='left')\n",
    "\n",
    "merged_df = merged_df.drop(columns=['city_prefix','rev_addr_city_id_y'])\n",
    "merged_df.rename(columns={'rev_addr_city_id_x': 'rev_addr_city_id'}, inplace=True)\n",
    "merged_df['city_level'] = merged_df['city_level'].fillna('六线城市')\n",
    "merged_2 = merged_df.copy()\n",
    "has_duplicates = merged_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddd2a022-b70a-45d2-aaca-bdc2aae1cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_df = pd.concat([merged_1, merged_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77cf0331-f293-48f4-af37-60e80917cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp=pd.read_csv('GDP-all.csv')\n",
    "gdp = gdp[['rev_addr_city_id','year','gdp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceda7326-a314-41dc-900b-a62c318ef683",
   "metadata": {},
   "outputs": [],
   "source": [
    "people=pd.read_csv('people-all.csv')\n",
    "people.rename(columns={'常住人口': 'people'}, inplace=True)\n",
    "people = people[['rev_addr_city_id','year','people']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bb882ae-4d14-44cc-a6a1-7646be9a94d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = final_merged_df.copy()\n",
    "df2 = people.copy()\n",
    "df3 = gdp.copy()\n",
    "merged_df_1 = pd.merge(df1, df2, on=['rev_addr_city_id','year'], how='left')\n",
    "merged_df_all = pd.merge(merged_df_1, df3, on=['rev_addr_city_id','year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "003c0e2b-4532-432c-aacc-183187772756",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nan_gdp = merged_df_all[(merged_df_all['people'].isna()) | (merged_df_all['gdp'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a2315df-b31d-4d46-a51f-c39451e4f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_exclude = test_nan_gdp['rev_addr_city_id'].unique()\n",
    "final_merged_df = merged_df_all[~merged_df_all['rev_addr_city_id'].isin(cities_to_exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c57f923-8ec7-4f75-a96a-f224d60ac432",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_number = data_all[~data_all['rev_addr_city_id'].isin(cities_to_exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0bccc32-235c-428d-bac8-80b590a16431",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_number_grouped_df = filter_number.groupby(['item_first_cate_name', 'year'])['nums'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a74af779-ecff-4e11-bbb7-15bb4be36615",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_number_grouped_df_2021 = filter_number_grouped_df[filter_number_grouped_df['year'] == 2021]\n",
    "filter_number_grouped_df_2021 = filter_number_grouped_df_2021[['item_first_cate_name','nums']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61581c36-52ea-4340-987e-3bad9240c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_number_grouped_df_2022 = filter_number_grouped_df[filter_number_grouped_df['year'] == 2022]\n",
    "filter_number_grouped_df_2022 = filter_number_grouped_df_2022[['item_first_cate_name','nums']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d23789c-5a76-4586-9dfb-43e6b2fd0834",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_merged_df)\n",
    "df = df[df['year'] == 2022]\n",
    "# Group by 'rev_addr_city_id' and count the occurrences of each city\n",
    "city_counts = df['city_level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48c21e28-b8f5-45a3-947e-47b71b782af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(final_merged_df)\n",
    "\n",
    "# Group by 'rev_addr_city_id' and count the occurrences of each city\n",
    "city_counts = df['rev_addr_city_id'].value_counts()\n",
    "\n",
    "# Find cities where the count is greater than 3\n",
    "cities_greater_than_3 = city_counts[city_counts > 2].index.tolist()\n",
    "\n",
    "cities_greater_than_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2f15367-3f69-4c93-a514-02eb331ef4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(final_merged_df)\n",
    "\n",
    "# Group by 'rev_addr_city_id' and count the occurrences of each city\n",
    "city_counts = df['rev_addr_city_id'].value_counts()\n",
    "\n",
    "# Find cities where the count is greater than 3\n",
    "cities_greater_than_3 = city_counts[city_counts < 1].index.tolist()\n",
    "\n",
    "cities_greater_than_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "853e9ce7-fcc6-427a-ba5e-568d87a020c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25663/1035597093.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_merged_df['city_level'] = final_merged_df['city_level'].replace(['一线城市', '新一线城市'], '一新一')\n"
     ]
    }
   ],
   "source": [
    "final_merged_df['city_level'] = final_merged_df['city_level'].replace(['一线城市', '新一线城市'], '一新一')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8c1b1a-b2eb-4470-b940-2504fff8abae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81e0a659-2601-40dd-9bd4-5fc39bd9172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exp_1 = final_merged_df.dropna()\n",
    "\n",
    "target_column = 'city_level'\n",
    "columns = [col for col in data_exp_1.columns if col != target_column]\n",
    "columns.insert(2, target_column)\n",
    "\n",
    "data_exp_1 = data_exp_1[columns]\n",
    "data_exp_1 = data_exp_1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "016a8605-4784-43cb-ae7e-7acf4a8a7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exp_2 = data_exp_1.dropna()\n",
    "\n",
    "data_exp_2['Sum'] = data_exp_2.iloc[:, 3:-2].sum(axis=1)\n",
    "\n",
    "data_exp_2 = data_exp_2[['rev_addr_city_id','year','city_level', 'Sum','people','gdp']]\n",
    "data_exp_2 = data_exp_2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d15dc1f9-75e1-4bc6-8c65-d2ad9db9aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exp_3 = data_exp_1.dropna()\n",
    "data_exp_3 = data_exp_3.drop(columns=['people'])\n",
    "data_exp_3 = data_exp_3.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05b11918-5ca6-49ac-8917-20d50440a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exp_4 = data_exp_2.dropna()\n",
    "data_exp_4 = data_exp_4.drop(columns=['people'])\n",
    "data_exp_4 = data_exp_4.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ae58843-428b-40cc-9f10-cd1cd2445d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exp_5 = final_merged_df.dropna()\n",
    "\n",
    "target_column = 'city_level'\n",
    "columns = [col for col in data_exp_5.columns if col != target_column]\n",
    "columns.insert(2, target_column)\n",
    "\n",
    "data_exp_5 = data_exp_5[columns]\n",
    "\n",
    "data_exp_5 = data_exp_5[['rev_addr_city_id','year','city_level','people','gdp']]\n",
    "\n",
    "data_exp_5 = data_exp_5.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4239b93-2136-4469-85d1-405dbee0333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from itertools import product\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.model_selection import (train_test_split, KFold, GroupKFold, StratifiedKFold,\n",
    "                                     StratifiedShuffleSplit, GridSearchCV)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a62b7b7-4726-43dc-8788-4bba1d1b8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "from itertools import product\n",
    "from tqdm import tqdm \n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def EN_CV(X_train, y_train, param_grid, stratifier):\n",
    "    best_r2_test = -np.inf\n",
    "    best_params = None\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    pbar = tqdm(total=skf.get_n_splits() * len(param_grid['alpha']) * len(param_grid['l1_ratio']))\n",
    "\n",
    "    for train_index, test_index in skf.split(X_train, stratifier):\n",
    "        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "        scaler_X = StandardScaler()\n",
    "        X_train_scaled = scaler_X.fit_transform(X_train_fold)\n",
    "        X_test_scaled = scaler_X.transform(X_test_fold)\n",
    "        \n",
    "        scaler_y = StandardScaler()\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train_fold.reshape(-1, 1)).ravel()\n",
    "        y_test_scaled = scaler_y.transform(y_test_fold.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        for alpha in param_grid['alpha']:\n",
    "            for l1_ratio in param_grid['l1_ratio']:\n",
    "                model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=10000)\n",
    "                model.fit(X_train_scaled, y_train_scaled)\n",
    "                y_test_pred_scaled = model.predict(X_test_scaled)\n",
    "                y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).ravel()\n",
    "                r2_test = r2_score(y_test_fold, y_test_pred)\n",
    "\n",
    "                if r2_test > best_r2_test:\n",
    "                    best_r2_test = r2_test\n",
    "                    best_params = {'alpha': alpha, 'l1_ratio': l1_ratio}\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return best_r2_test, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5594906b-22d3-4388-934c-ed1eed669764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_sort_dataframes(df1, df2, ascending=False):\n",
    "    df1.rename(columns={'Feature': 'item_first_cate_name'}, inplace=True)\n",
    "\n",
    "    merged_df = pd.merge(df1[['item_first_cate_name', 'Weight']],\n",
    "                         df2[['item_first_cate_name', 'nums']],\n",
    "                         on='item_first_cate_name', how='outer')\n",
    "    \n",
    "    merged_df['Weight_abs'] = merged_df['Weight'].abs() \n",
    "    sorted_merged_df = merged_df.sort_values(by='Weight_abs', ascending=ascending)\n",
    "    \n",
    "    sorted_merged_df['Weight_Rank'] = sorted_merged_df['Weight'].abs().rank(ascending=ascending, method='average')  \n",
    "    sorted_merged_df['Nums_Rank'] = sorted_merged_df['nums'].rank(ascending=False, method='average')  \n",
    "\n",
    "    return sorted_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f53bbedd-c0bb-418c-86fb-c1faf7b03f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "\n",
    "def EN_evaluate(data, model_name, param_grid, year):\n",
    "    random_seed = 51\n",
    "    data_1 = data[data['year'] == year]\n",
    "    data_1 = data_1.reset_index(drop=True)\n",
    "    metrics = []\n",
    "    best_r2_test = -np.inf  \n",
    "    best_weights = None  \n",
    "    feature_names = data.columns[3:-1]\n",
    "    X = data_1.iloc[:, 3:-1].values\n",
    "    y = data_1.iloc[:, -1].values\n",
    "    \n",
    "    data_2 = data[data['year'] == year + 1]\n",
    "    data_2 = data_2.reset_index(drop=True)\n",
    "    X_1 = data_2.iloc[:, 3:-1].values\n",
    "    y_1 = data_2.iloc[:, -1].values\n",
    "    \n",
    "    stratifier = data_1['city_level']\n",
    "    \n",
    "    X_train, X_test, y_train, _, strat_train, strat_test = train_test_split(X, y, stratifier, test_size=0.3, stratify=stratifier, random_state=random_seed)\n",
    "    _, _, _, y_test, _, _ = train_test_split(X_1, y_1, stratifier, test_size=0.3, stratify=stratifier, random_state=random_seed)\n",
    "\n",
    "    \n",
    "    r2_test, best_params = EN_CV(X_train, y_train, param_grid, strat_train)\n",
    "\n",
    "    scaler_X = StandardScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "    scaler_y = StandardScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "    y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n",
    "                    \n",
    "    xg_reg = ElasticNet(alpha=best_params['alpha'], l1_ratio=best_params['l1_ratio'], max_iter=10000)\n",
    "    \n",
    "    xg_reg.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "    y_train_pred_scaled = xg_reg.predict(X_train_scaled)\n",
    "    y_test_pred_scaled = xg_reg.predict(X_test_scaled)\n",
    "    \n",
    "\n",
    "    y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).ravel()\n",
    "    y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    best_r2_test = r2_test\n",
    "    best_weights = xg_reg.coef_\n",
    "\n",
    "    fold_metrics = {\n",
    "        'RMSE_train': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'MAPE_train': mean_absolute_percentage_error(y_train, y_train_pred),\n",
    "        'MAE_train': mean_absolute_error(y_train, y_train_pred),\n",
    "        'R2_train': r2_score(y_train, y_train_pred),\n",
    "        \n",
    "        'RMSE_test': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        'MAPE_test': mean_absolute_percentage_error(y_test, y_test_pred),\n",
    "        'MAE_test': mean_absolute_error(y_test, y_test_pred),\n",
    "        'R2_test': r2_score(y_test, y_test_pred)\n",
    "    }\n",
    "        \n",
    "        \n",
    "    metrics.append({**fold_metrics})\n",
    "\n",
    "    results_df = pd.DataFrame(metrics)\n",
    "\n",
    "    results_df.insert(0, 'Data', model_name)\n",
    "\n",
    "    best_weights_df = pd.DataFrame({'Feature': feature_names, 'Weight': best_weights})\n",
    "    return best_weights_df, results_df,best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab43c538-4c60-42cf-8afd-b353e08d1aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3030/3030 [01:07<00:00, 44.61it/s] \n",
      "100%|██████████| 3030/3030 [00:08<00:00, 368.62it/s]\n",
      "100%|██████████| 3030/3030 [01:08<00:00, 44.31it/s] \n",
      "100%|██████████| 3030/3030 [00:07<00:00, 401.25it/s]\n",
      "100%|██████████| 3030/3030 [00:07<00:00, 409.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "EN_param_grid = {\n",
    "    'alpha': [0.00,0.01,0.1,1.0, 5.0, 10], # , 5.0, 10.0, 20.0, 50.0, 100, 200\n",
    "    'l1_ratio': np.arange(0, 1.01, 0.01)\n",
    "}\n",
    "dataframes = {\n",
    "    'data_exp_1': data_exp_1,\n",
    "    'data_exp_2': data_exp_2,\n",
    "    'data_exp_3': data_exp_3,\n",
    "    'data_exp_4': data_exp_4,\n",
    "    'data_exp_5': data_exp_5\n",
    "}\n",
    "\n",
    "years = [2021]\n",
    "\n",
    "EN_results = pd.DataFrame()\n",
    "EN_weights = pd.DataFrame()\n",
    "EN_param = pd.DataFrame()\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    for year in years:\n",
    "        average_weights, result_df, best_params = EN_evaluate(df, name, EN_param_grid, year)\n",
    "        \n",
    "        result_df['Data'] = f\"{name}_{year}\"\n",
    "        EN_results = pd.concat([EN_results, result_df], ignore_index=True)\n",
    "\n",
    "        average_weights['Experiment'] = f\"{name}_{year}\"\n",
    "        EN_weights = pd.concat([EN_weights, average_weights], ignore_index=True)\n",
    "\n",
    "        best_params_df = pd.DataFrame([best_params])\n",
    "        best_params_df['Experiment'] = f\"{name}_{year}\"\n",
    "        EN_param = pd.concat([EN_param, best_params_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f40e4e8-6a3c-4b20-befa-163c94d40582",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_2021 = EN_results[EN_results['Data'].str.contains('2021')]\n",
    "all_results_2022 = EN_results[EN_results['Data'].str.contains('2022')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38bfa0fe-b092-4e60-8430-5e34767bd180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_sort_dataframes(df1, df2, ascending=False):\n",
    "    df1.rename(columns={'Feature': 'item_first_cate_name'}, inplace=True)\n",
    "\n",
    "    merged_df = pd.merge(df1[['item_first_cate_name', 'Weight']],\n",
    "                         df2[['item_first_cate_name', 'nums']],\n",
    "                         on='item_first_cate_name', how='outer')\n",
    "    \n",
    "\n",
    "    merged_df['Weight_abs'] = merged_df['Weight'].abs()  \n",
    "    sorted_merged_df = merged_df.sort_values(by='Weight_abs', ascending=ascending)\n",
    "    \n",
    "    sorted_merged_df['Weight_Rank'] = sorted_merged_df['Weight'].abs().rank(ascending=ascending, method='average')  \n",
    "    sorted_merged_df['Nums_Rank'] = sorted_merged_df['nums'].rank(ascending=False, method='average')  \n",
    "\n",
    "    return sorted_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd351f3d-362b-497a-bb1a-0e9639aabc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_weights_data(EN_weights, year, experiment_name, grouped_df):\n",
    "\n",
    "    filtered_weights = EN_weights[EN_weights['Experiment'].str.contains(year)]\n",
    "    filtered_weights = filtered_weights[filtered_weights['Experiment'].str.contains(experiment_name)]\n",
    "    \n",
    "    merged_data = merge_and_sort_dataframes(filtered_weights, grouped_df)\n",
    "    \n",
    "    result_df = merged_data[['item_first_cate_name', 'Weight', 'Weight_Rank', 'nums', 'Nums_Rank']]\n",
    "\n",
    "    result_df.loc[:, 'nums'] = result_df['nums'].round(1)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "271350c3-eba6-4504-9d61-cf7458aea655",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"2021\"\n",
    "experiment_name = \"data_exp_1\"\n",
    "exp1_EN_weights_2021 = process_weights_data(EN_weights, year, experiment_name, filter_number_grouped_df_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be999c75-be1e-49c2-a1bf-0325dc5b136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"2021\"\n",
    "experiment_name = \"data_exp_3\"\n",
    "exp3_EN_weights_2021 = process_weights_data(EN_weights, year, experiment_name, filter_number_grouped_df_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35bca2c9-049a-4ca0-9de8-7ca836ef0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_1 = exp1_EN_weights_2021.head(11).copy()\n",
    "TEST_2 = exp3_EN_weights_2021.head(10).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28f2b6eb-c7e2-49e7-8f99-d11f96e88afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_1 = TEST_1[TEST_1['Nums_Rank'] > 10]\n",
    "TEST_2 = TEST_2[TEST_2['Nums_Rank'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80784024-15ec-4c17-86b0-b1d39514a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(TEST_1)\n",
    "df2 = pd.DataFrame(TEST_2)\n",
    "\n",
    "common_names = set(df1['item_first_cate_name']).intersection(df2['item_first_cate_name'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
